{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9pY/p7DO/l5zkH4wER2EW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun9438/Boston-Housing-Pricing/blob/main/NLP_introducion_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "Computational Linguistics is an interdisciplinary field that focuses on modeling, analyzing, and understanding human language using computational methods. It combines linguistics, computer science, artificial intelligence, and mathematics to study language structure and meaning.\n",
        "\n",
        "Natural Language Processing (NLP) is an applied subfield of computational linguistics that focuses on enabling machines to process, understand, and generate human language. While computational linguistics emphasizes theoretical models of language, NLP focuses on building practical applications such as chatbots, translation systems, sentiment analysis tools, and speech recognition systems.\n",
        "\n",
        "Thus, computational linguistics provides the theoretical foundation, while NLP applies these theories to real-world problems."
      ],
      "metadata": {
        "id": "xXiNHYBwJfSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "-The evolution of NLP can be divided into several phases:\n",
        "\t1.\t1950s–1960s (Rule-Based Era): Early NLP systems relied on hand-crafted linguistic rules. The Georgetown-IBM experiment (1954) demonstrated basic machine translation.\n",
        "\t2.\t1970s–1980s (Symbolic & Statistical Methods): Researchers introduced probabilistic models and corpus-based approaches to overcome limitations of rule-based systems.\n",
        "\t3.\t1990s (Machine Learning Era): NLP systems began using machine learning techniques such as Hidden Markov Models (HMMs) and Naive Bayes for tasks like POS tagging and speech recognition.\n",
        "\t4.\t2000s–2010s (Deep Learning Era): Neural networks, word embeddings (Word2Vec, GloVe), and deep learning models improved NLP performance significantly.\n",
        "\t5.\tRecent Advances: Transformer-based models like BERT and GPT revolutionized NLP by enabling context-aware language understanding and generation.\n"
      ],
      "metadata": {
        "id": "V-ISwtbIJmx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "- 1.\tChatbots and Virtual Assistants:\n",
        "NLP enables conversational interfaces such as customer support chatbots and voice assistants to understand and respond to user queries.\n",
        "-\t2.\tSentiment Analysis:\n",
        "Companies analyze customer reviews, social media posts, and feedback to determine public sentiment and improve products or services.\n",
        "-\t3.\tMachine Translation:\n",
        "NLP powers translation tools that convert text from one language to another while preserving meaning and context.\n"
      ],
      "metadata": {
        "id": "ujgIg2DMJ4k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "-Text normalization is the process of converting text into a consistent and standardized format. It includes operations such as converting text to lowercase, removing punctuation, correcting spelling, expanding contractions, and removing stopwords.\n",
        "\n",
        "Text normalization is essential because raw text data is often noisy and inconsistent. Normalization reduces complexity, improves model accuracy, and ensures that different forms of the same word are treated uniformly during processing."
      ],
      "metadata": {
        "id": "5xz1wo6YKBWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: Compare and contrast stemming and lemmatization with suitable examples.\n",
        "-Feature\n",
        "Stemming\n",
        "Lemmatization\n",
        "Definition\n",
        "Removes word endings\n",
        "Converts words to base dictionary form\n",
        "Accuracy\n",
        "Less accurate\n",
        "More accurate\n",
        "Linguistic knowledge\n",
        "Not required\n",
        "Required\n",
        "Speed\n",
        "Faster\n",
        "Slower\n"
      ],
      "metadata": {
        "id": "2lkNca3ZKG5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Regex program to extract email addresses"
      ],
      "metadata": {
        "id": "AAzG73pXKRIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "emails = re.findall(r'[a-zA-Z0-9_.]+@[a-zA-Z0-9.]+\\.[a-zA-Z]+', text)\n",
        "print(emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpnfn30CJbV9",
        "outputId": "fffdb1f1-6217-4404-9a50-686ba2861b72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Tokenization and Frequency Distribution using NLTK"
      ],
      "metadata": {
        "id": "s5oZf4wiKZnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download missing resource\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines\n",
        "linguistics, computer science, and artificial intelligence.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T0baBZIKUHH",
        "outputId": "ee319083-54c1-4f68-a90a-0af489d08e88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 20 samples and 21 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Train Word2Vec using Gensim"
      ],
      "metadata": {
        "id": "SyM7iv6mK04x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation\",\n",
        " \"Word2Vec is a popular word embedding technique\",\n",
        " \"Text preprocessing is a critical step\",\n",
        " \"Tokenization and normalization help clean text\"\n",
        "]\n",
        "\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "model = Word2Vec(tokenized_data, vector_size=50, window=5, min_count=1, workers=2)\n",
        "\n",
        "print(model.wv['language'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn-SF3Q_KdaN",
        "outputId": "dcdbce9a-d49c-4629-b679-c399cd871ac0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "[ 0.00018913  0.00615464 -0.01362529 -0.00275093  0.01533716  0.01469282\n",
            " -0.00734659  0.0052854  -0.01663426  0.01241097 -0.00927464 -0.00632821\n",
            "  0.01862271  0.00174677  0.01498141 -0.01214813  0.01032101  0.01984565\n",
            " -0.01691478 -0.01027138 -0.01412967 -0.0097253  -0.00755713 -0.0170724\n",
            "  0.01591121 -0.00968788  0.01684723  0.01052514 -0.01310005  0.00791574\n",
            "  0.0109403  -0.01485307 -0.01481144 -0.00495046 -0.01725145 -0.00316314\n",
            " -0.00080687  0.00659937  0.00288376 -0.00176284 -0.01118812  0.00346073\n",
            " -0.00179474  0.01358738  0.00794718  0.00905894  0.00286861 -0.00539971\n",
            " -0.00873363 -0.00206415]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: NLP pipeline for analyzing fintech customer feedback\n",
        "1.\tData collection\n",
        "2.\tText cleaning & normalization\n",
        "3.\tTokenization\n",
        "4.\tStopword removal\n",
        "5.\tLemmatization\n",
        "6.\tSentiment analysis\n",
        "7.\tTopic modeling\n",
        "8.\tInsight extraction & visualization"
      ],
      "metadata": {
        "id": "K86FCkGzLAzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "reviews = [\"The app is excellent and easy to use\", \"Customer support is very poor\"]\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "for review in reviews:\n",
        "    print(review, sia.polarity_scores(review))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPWnQzQNK4J6",
        "outputId": "2d16f525-b722-475f-aee5-670ff19aa4bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The app is excellent and easy to use {'neg': 0.0, 'neu': 0.476, 'pos': 0.524, 'compound': 0.765}\n",
            "Customer support is very poor {'neg': 0.373, 'neu': 0.33, 'pos': 0.297, 'compound': -0.1761}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vFCcbmCLSsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}