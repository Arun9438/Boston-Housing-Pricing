{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP70/dJ8uy0u9BsvX1UZiD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun9438/Boston-Housing-Pricing/blob/main/CNN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "lmvnukeKvQUc",
        "outputId": "6ec60f58-dcb9-4f2c-96d2-cd3866afd406"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is a CNN? Define a CNN as a specialized type of neural network primarily designed to process data with a known grid-like topology, such as image pixels. Highlight its key layers: Convolutional Layer, Pooling Layer, and Fully Connected Layer.\\n\\nArchitectural Differences:\\n\\nConnectivity: Traditional Fully Connected Networks (FCNs) have every neuron connected to every neuron in the next layer. CNNs use sparse connectivity (local receptive fields) and weight sharing (filters/kernels) in the convolutional layers.\\n\\nInput Handling: FCNs flatten the 2D or 3D image into a 1D vector, losing spatial information. CNNs process the input as a 2D or 3D volume, preserving the spatial hierarchy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?\n",
        "\n",
        "'''What is a CNN? Define a CNN as a specialized type of neural network primarily designed to process data with a known grid-like topology, such as image pixels. Highlight its key layers: Convolutional Layer, Pooling Layer, and Fully Connected Layer.\n",
        "\n",
        "Architectural Differences:\n",
        "\n",
        "Connectivity: Traditional Fully Connected Networks (FCNs) have every neuron connected to every neuron in the next layer. CNNs use sparse connectivity (local receptive fields) and weight sharing (filters/kernels) in the convolutional layers.\n",
        "\n",
        "Input Handling: FCNs flatten the 2D or 3D image into a 1D vector, losing spatial information. CNNs process the input as a 2D or 3D volume, preserving the spatial hierarchy.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision.\n",
        "'''LeNet-5 Architecture Description: Detail the structure of LeNet-5 (designed by Yann LeCun in the late 1990s for digit classification, notably MNIST). Describe its key stages:\n",
        "\n",
        "Input: 32x32 image.\n",
        "\n",
        "C1 (Convolutional): Convolutional layer with 6 filters.\n",
        "\n",
        "S2 (Subsampling/Pooling): Average pooling.\n",
        "\n",
        "C3 (Convolutional): Convolutional layer with 16 filters.\n",
        "\n",
        "S4 (Subsampling/Pooling): Average pooling.\n",
        "\n",
        "C5 (Convolutional/Fully Connected): 120 units.\n",
        "\n",
        "F6 (Fully Connected): 84 units.\n",
        "\n",
        "Output: 10 units (for classification).\n",
        "\n",
        "Foundational Contributions: Explain how LeNet-5 pioneered key concepts that are standard in modern CNNs:\n",
        "\n",
        "The sequential application of convolutional layers, followed by pooling layers (the C-P/C-S pattern).\n",
        "\n",
        "The use of shared weights (convolution) to extract features efficiently.\n",
        "\n",
        "The complete system from raw pixel input to final classification.\n",
        "\n",
        "Reference: Cite the original paper: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "02b41nzlvfwb",
        "outputId": "2aecc06e-bbd8-403c-d731-ed6b5f79d338"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LeNet-5 Architecture Description: Detail the structure of LeNet-5 (designed by Yann LeCun in the late 1990s for digit classification, notably MNIST). Describe its key stages:\\n\\nInput: 32x32 image.\\n\\nC1 (Convolutional): Convolutional layer with 6 filters.\\n\\nS2 (Subsampling/Pooling): Average pooling.\\n\\nC3 (Convolutional): Convolutional layer with 16 filters.\\n\\nS4 (Subsampling/Pooling): Average pooling.\\n\\nC5 (Convolutional/Fully Connected): 120 units.\\n\\nF6 (Fully Connected): 84 units.\\n\\nOutput: 10 units (for classification).\\n\\nFoundational Contributions: Explain how LeNet-5 pioneered key concepts that are standard in modern CNNs:\\n\\nThe sequential application of convolutional layers, followed by pooling layers (the C-P/C-S pattern).\\n\\nThe use of shared weights (convolution) to extract features efficiently.\\n\\nThe complete system from raw pixel input to final classification.\\n\\nReference: Cite the original paper: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What is transfer learning in the context of image classification?\n",
        "'''Definition of Transfer Learning: Explain it as a machine learning method where a model developed for a task (the source task, e.g., classifying 1000 categories on ImageNet) is reused as the starting point for a model on a different but related task (the target task, e.g., classifying medical X-rays).\n",
        "\n",
        "Mechanism in Image Classification: Typically involves taking a pre-trained CNN (like VGG, ResNet, etc.) and:\n",
        "\n",
        "Freezing the early convolutional layers (since they've learned general features like edges and textures).\n",
        "\n",
        "Replacing the final classification layer(s) with new layers tailored to the target task's classes.\n",
        "\n",
        "Training the new final layers (and optionally fine-tuning the later convolutional layers).\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Reduces Computational Costs: Training starts from an already optimized point, drastically reducing the time and resources needed for convergence, as opposed to training a deep network from scratch.\n",
        "\n",
        "Improves Performance with Limited Data: The pre-trained model has already learned highly relevant features from a massive dataset (like ImageNet). These features are useful for the new task, mitigating the risk of overfitting that occurs when training a large model on a small, limited dataset. The model benefits from the \"experience\" gained on the source task.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "5sCW_dZcvouT",
        "outputId": "ccab925b-4ae4-4b5f-d56b-1f82f344bba4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Definition of Transfer Learning: Explain it as a machine learning method where a model developed for a task (the source task, e.g., classifying 1000 categories on ImageNet) is reused as the starting point for a model on a different but related task (the target task, e.g., classifying medical X-rays).\\n\\nMechanism in Image Classification: Typically involves taking a pre-trained CNN (like VGG, ResNet, etc.) and:\\n\\nFreezing the early convolutional layers (since they\\'ve learned general features like edges and textures).\\n\\nReplacing the final classification layer(s) with new layers tailored to the target task\\'s classes.\\n\\nTraining the new final layers (and optionally fine-tuning the later convolutional layers).\\n\\nBenefits:\\n\\nReduces Computational Costs: Training starts from an already optimized point, drastically reducing the time and resources needed for convergence, as opposed to training a deep network from scratch.\\n\\nImproves Performance with Limited Data: The pre-trained model has already learned highly relevant features from a massive dataset (like ImageNet). These features are useful for the new task, mitigating the risk of overfitting that occurs when training a large model on a small, limited dataset. The model benefits from the \"experience\" gained on the source task.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Describe the role of residual connections in ResNet architecture.\n",
        "'''ResNet and Residual Blocks: Introduce ResNet (Residual Network) and its core building block:\n",
        "the Residual Block.Role of Residual Connections: The residual connection (or skip connection) adds the input of the block,\n",
        "$x$, to the output of the layer sequence, $F(x)$, resulting in the output $H(x) = F(x) + x$8.$F(x)$ represents the residual mapping that the block is trying to learn. The network learns $F(x)$ instead of the full mapping $H(x)$.Addressing the Vanishing Gradient Problem:The Problem: In very deep networks, the gradient signal during backpropagation must pass through many layers, leading to the gradient magnitude shrinking exponentially (vanishing) as it moves toward the initial layers. This stops early layers from learning.The Solution: The skip connection creates a direct pathway for the gradient to flow through backpropagation9. The derivative of the identity mapping $x$ is 1. When the gradient flows back through the block, it includes a term from the bypass path: $\\frac{\\partial H}{\\partial x} = \\frac{\\partial F}{\\partial x} + 1$.The \"+1\" term ensures that the gradient is never completely attenuated,\n",
        "mitigating the vanishing gradient effect and allowing for the successful training of models with hundreds of layers.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "Rdl-fVlRv39B",
        "outputId": "2fd69818-211d-4aa0-ffe5-765f87a55b8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\p'\n",
            "/tmp/ipython-input-2545706225.py:4: SyntaxWarning: invalid escape sequence '\\p'\n",
            "  $x$, to the output of the layer sequence, $F(x)$, resulting in the output $H(x) = F(x) + x$8.$F(x)$ represents the residual mapping that the block is trying to learn. The network learns $F(x)$ instead of the full mapping $H(x)$.Addressing the Vanishing Gradient Problem:The Problem: In very deep networks, the gradient signal during backpropagation must pass through many layers, leading to the gradient magnitude shrinking exponentially (vanishing) as it moves toward the initial layers. This stops early layers from learning.The Solution: The skip connection creates a direct pathway for the gradient to flow through backpropagation9. The derivative of the identity mapping $x$ is 1. When the gradient flows back through the block, it includes a term from the bypass path: $\\frac{\\partial H}{\\partial x} = \\frac{\\partial F}{\\partial x} + 1$.The \"+1\" term ensures that the gradient is never completely attenuated,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ResNet and Residual Blocks: Introduce ResNet (Residual Network) and its core building block: \\nthe Residual Block.Role of Residual Connections: The residual connection (or skip connection) adds the input of the block, \\n$x$, to the output of the layer sequence, $F(x)$, resulting in the output $H(x) = F(x) + x$8.$F(x)$ represents the residual mapping that the block is trying to learn. The network learns $F(x)$ instead of the full mapping $H(x)$.Addressing the Vanishing Gradient Problem:The Problem: In very deep networks, the gradient signal during backpropagation must pass through many layers, leading to the gradient magnitude shrinking exponentially (vanishing) as it moves toward the initial layers. This stops early layers from learning.The Solution: The skip connection creates a direct pathway for the gradient to flow through backpropagation9. The derivative of the identity mapping $x$ is 1. When the gradient flows back through the block, it includes a term from the bypass path: $\\x0crac{\\\\partial H}{\\\\partial x} = \\x0crac{\\\\partial F}{\\\\partial x} + 1$.The \"+1\" term ensures that the gradient is never completely attenuated, \\nmitigating the vanishing gradient effect and allowing for the successful training of models with hundreds of layers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "\n",
        "# 1. Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# LeNet expects 32x32, so we pad 28x28 to 32x32\n",
        "# Cast to tf.float32 before division for normalization\n",
        "x_train = tf.pad(tf.expand_dims(x_train, -1), [[0,0],[2,2],[2,2],[0,0]])\n",
        "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
        "x_test  = tf.pad(tf.expand_dims(x_test, -1),  [[0,0],[2,2],[2,2],[0,0]])\n",
        "x_test = tf.cast(x_test, tf.float32) / 255.0\n",
        "\n",
        "print(\"Train shape:\", x_train.shape)  # (60000, 32, 32, 1)\n",
        "print(\"Test shape:\", x_test.shape)    # (10000, 32, 32, 1)\n",
        "\n",
        "# 2. Build LeNet-5-like model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(6, (5, 5), activation='tanh', input_shape=(32, 32, 1)),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(16, (5, 5), activation='tanh'),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(120, (5, 5), activation='tanh'),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(84, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 3. Train and measure time\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "train_time = end_time - start_time\n",
        "print(f\"Total training time: {train_time:.2f} seconds\")\n",
        "\n",
        "# 4. Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "0lIxqhrewGbM",
        "outputId": "56323fa9-e530-4dc6-b2da-336db415921f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (60000, 32, 32, 1)\n",
            "Test shape: (10000, 32, 32, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m120\u001b[0m)      │        \u001b[38;5;34m48,120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,706\u001b[0m (241.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,706</span> (241.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "422/422 - 25s - 60ms/step - accuracy: 0.8960 - loss: 0.3561 - val_accuracy: 0.9615 - val_loss: 0.1349\n",
            "Epoch 2/10\n",
            "422/422 - 23s - 55ms/step - accuracy: 0.9620 - loss: 0.1256 - val_accuracy: 0.9743 - val_loss: 0.0854\n",
            "Epoch 3/10\n",
            "422/422 - 40s - 94ms/step - accuracy: 0.9757 - loss: 0.0799 - val_accuracy: 0.9820 - val_loss: 0.0675\n",
            "Epoch 4/10\n",
            "422/422 - 40s - 95ms/step - accuracy: 0.9824 - loss: 0.0587 - val_accuracy: 0.9807 - val_loss: 0.0579\n",
            "Epoch 5/10\n",
            "422/422 - 46s - 109ms/step - accuracy: 0.9849 - loss: 0.0478 - val_accuracy: 0.9827 - val_loss: 0.0575\n",
            "Epoch 6/10\n",
            "422/422 - 28s - 66ms/step - accuracy: 0.9884 - loss: 0.0378 - val_accuracy: 0.9853 - val_loss: 0.0511\n",
            "Epoch 7/10\n",
            "422/422 - 38s - 90ms/step - accuracy: 0.9904 - loss: 0.0320 - val_accuracy: 0.9857 - val_loss: 0.0495\n",
            "Epoch 8/10\n",
            "422/422 - 26s - 61ms/step - accuracy: 0.9920 - loss: 0.0256 - val_accuracy: 0.9882 - val_loss: 0.0440\n",
            "Epoch 9/10\n",
            "422/422 - 41s - 97ms/step - accuracy: 0.9931 - loss: 0.0226 - val_accuracy: 0.9880 - val_loss: 0.0435\n",
            "Epoch 10/10\n",
            "422/422 - 24s - 57ms/step - accuracy: 0.9939 - loss: 0.0188 - val_accuracy: 0.9852 - val_loss: 0.0511\n",
            "Total training time: 348.11 seconds\n",
            "Test accuracy: 0.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "\n",
        "# Setup for a sample dataset (flower_photos)\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "# get_file returns the path to the extracted directory (e.g., /root/.keras/datasets/flower_photos)\n",
        "downloaded_data_path = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "\n",
        "# The actual image directories (daisy, dandelion, etc.) are often inside a subdirectory\n",
        "# named 'flower_photos' within the downloaded_data_path after extraction.\n",
        "# So, adjust data_dir to point to the folder *containing* the class subdirectories.\n",
        "data_dir = pathlib.Path(downloaded_data_path)\n",
        "if (data_dir / 'flower_photos').is_dir():\n",
        "    data_dir = data_dir / 'flower_photos'\n",
        "\n",
        "print(f\"Adjusted data_dir pointing to class folders: {data_dir}\")\n",
        "if not data_dir.is_dir():\n",
        "    raise FileNotFoundError(f\"Adjusted data_dir does not exist: {data_dir}\")\n",
        "\n",
        "class_folders = [f for f in os.listdir(data_dir) if (data_dir / f).is_dir()]\n",
        "if not class_folders:\n",
        "    raise ValueError(f\"No class subfolders found in {data_dir}. Contents: {os.listdir(data_dir)}\")\n",
        "print(f\"Found class folders: {class_folders}\")\n",
        "\n",
        "\n",
        "# Create train and validation directories\n",
        "base_data_dir = 'data'\n",
        "train_dir = os.path.join(base_data_dir, 'train')\n",
        "val_dir = os.path.join(base_data_dir, 'val')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Split data into train and validation (approx 80/20 split)\n",
        "total_images_copied = 0\n",
        "for class_name in class_folders:\n",
        "    class_source_path = data_dir / class_name\n",
        "    images = list(class_source_path.glob('*.jpg')) # Look for .jpg files\n",
        "    if not images:\n",
        "        # If no .jpg found, try .jpeg\n",
        "        images = list(class_source_path.glob('*.jpeg'))\n",
        "        if images:\n",
        "            print(f\"Found .jpeg images for class {class_name}\")\n",
        "        else:\n",
        "            print(f\"No .jpg or .jpeg images found in {class_source_path}. Skipping class.\")\n",
        "            continue\n",
        "\n",
        "    num_images = len(images)\n",
        "    num_train = int(0.8 * num_images)\n",
        "    total_images_copied += num_images\n",
        "\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "\n",
        "    for i, img_path in enumerate(images):\n",
        "        if i < num_train:\n",
        "            shutil.copy(img_path, os.path.join(train_dir, class_name, img_path.name))\n",
        "        else:\n",
        "            shutil.copy(img_path, os.path.join(val_dir, class_name, img_path.name))\n",
        "\n",
        "print(f\"Finished copying images. Total images processed: {total_images_copied}\")\n",
        "if total_images_copied == 0:\n",
        "    raise ValueError(\"No images were copied to train/val directories. Data splitting failed.\")\n",
        "\n",
        "\n",
        "img_size = (224, 224)\n",
        "batch_size = 16\n",
        "# Dynamically determine num_classes from the created training directories\n",
        "num_classes = len([name for name in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, name))])\n",
        "if num_classes == 0:\n",
        "    raise ValueError(\"No classes found in the training directory after splitting. Data generators will fail.\")\n",
        "\n",
        "\n",
        "# 1. Data generators with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 2. Load pre-trained VGG16 without top classifier\n",
        "base_model = VGG16(weights='imagenet', include_top=False,\n",
        "                   input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze most layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 3. Add custom top layers\n",
        "x = layers.Flatten()(base_model.output)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x) # Use dynamic num_classes\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 4. Train the top layers\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=10,\n",
        "    validation_data=val_gen\n",
        ")\n",
        "\n",
        "# Optionally unfreeze some deeper layers and fine-tune\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=5,\n",
        "    validation_data=val_gen\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3RCP6pZfw2a_",
        "outputId": "a59cdfdb-4c21-4ba0-8e29-643af11c4c1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "\u001b[1m228813984/228813984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
            "Adjusted data_dir pointing to class folders: /root/.keras/datasets/flower_photos/flower_photos\n",
            "Found class folders: ['sunflowers', 'roses', 'dandelion', 'daisy', 'tulips']\n",
            "Finished copying images. Total images processed: 3670\n",
            "Found 2934 images belonging to 5 classes.\n",
            "Found 736 images belonging to 5 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m6,422,784\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m1,285\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,138,757\u001b[0m (80.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,138,757</span> (80.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,424,069\u001b[0m (24.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,424,069</span> (24.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 280ms/step - accuracy: 0.4655 - loss: 1.3597 - val_accuracy: 0.7663 - val_loss: 0.7107\n",
            "Epoch 2/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 251ms/step - accuracy: 0.7253 - loss: 0.7607 - val_accuracy: 0.7840 - val_loss: 0.5893\n",
            "Epoch 3/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 255ms/step - accuracy: 0.7546 - loss: 0.6817 - val_accuracy: 0.8179 - val_loss: 0.5196\n",
            "Epoch 4/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 255ms/step - accuracy: 0.7596 - loss: 0.6333 - val_accuracy: 0.8302 - val_loss: 0.5006\n",
            "Epoch 5/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 255ms/step - accuracy: 0.7903 - loss: 0.5672 - val_accuracy: 0.8302 - val_loss: 0.5018\n",
            "Epoch 6/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 253ms/step - accuracy: 0.8150 - loss: 0.5259 - val_accuracy: 0.8383 - val_loss: 0.4730\n",
            "Epoch 7/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 253ms/step - accuracy: 0.8246 - loss: 0.4834 - val_accuracy: 0.8519 - val_loss: 0.4629\n",
            "Epoch 8/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 256ms/step - accuracy: 0.8405 - loss: 0.4603 - val_accuracy: 0.8424 - val_loss: 0.4606\n",
            "Epoch 9/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 253ms/step - accuracy: 0.8304 - loss: 0.4592 - val_accuracy: 0.8356 - val_loss: 0.4855\n",
            "Epoch 10/10\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 253ms/step - accuracy: 0.8290 - loss: 0.4544 - val_accuracy: 0.8533 - val_loss: 0.4520\n",
            "Epoch 1/5\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 273ms/step - accuracy: 0.8617 - loss: 0.3734 - val_accuracy: 0.8614 - val_loss: 0.4139\n",
            "Epoch 2/5\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 259ms/step - accuracy: 0.9038 - loss: 0.2866 - val_accuracy: 0.8845 - val_loss: 0.3778\n",
            "Epoch 3/5\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 260ms/step - accuracy: 0.9264 - loss: 0.2335 - val_accuracy: 0.8845 - val_loss: 0.3835\n",
            "Epoch 4/5\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 260ms/step - accuracy: 0.9307 - loss: 0.2123 - val_accuracy: 0.8913 - val_loss: 0.3679\n",
            "Epoch 5/5\n",
            "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 261ms/step - accuracy: 0.9300 - loss: 0.1950 - val_accuracy: 0.8777 - val_loss: 0.3908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf # Import tensorflow for downloading utility\n",
        "\n",
        "# 1. Load pre-trained AlexNet\n",
        "alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
        "alexnet.eval()\n",
        "\n",
        "# First convolutional layer\n",
        "first_conv = alexnet.features[0]  # Conv2d\n",
        "\n",
        "# 2. Visualize filters\n",
        "weights = first_conv.weight.data.clone()  # shape: (64, 3, 11, 11)\n",
        "\n",
        "def normalize(t):\n",
        "    t = t - t.min()\n",
        "    t = t / (t.max() + 1e-5)\n",
        "    return t\n",
        "\n",
        "num_filters = 16  # visualize first 16 filters\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(num_filters):\n",
        "    f = weights[i]\n",
        "    f = normalize(f)\n",
        "    f = f.permute(1, 2, 0).numpy()  # C,H,W -> H,W,C\n",
        "    plt.subplot(2, 8, i + 1)\n",
        "    plt.imshow(f)\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"First Conv Layer Filters (AlexNet)\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Load example image and preprocess\n",
        "# Download a sample image for demonstration\n",
        "img_path = tf.keras.utils.get_file(\n",
        "    'elephant.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/e/e4/African_bush_elephant_%28Loxodonta_africana%29_bull%2C_Phinda_Private_Game_Reserve%2C_KwaZulu-Natal%2C_South_Africa.jpg' # Updated URL to a more reliable source\n",
        ")\n",
        "\n",
        "input_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=models.AlexNet_Weights.IMAGENET1K_V1.meta['mean'],\n",
        "        std=models.AlexNet_Weights.IMAGENET1K_V1.meta['std']\n",
        "    )\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image).unsqueeze(0)  # shape (1,3,224,224)\n",
        "\n",
        "# 4. Forward pass through first conv layer\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(input_tensor)  # (1, 64, H, W)\n",
        "\n",
        "feature_maps = feature_maps.squeeze(0)  # (64, H, W)\n",
        "\n",
        "# 5. Visualize some feature maps\n",
        "num_maps = 16\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(num_maps):\n",
        "    fm = feature_maps[i]\n",
        "    fm = normalize(fm)\n",
        "    plt.subplot(2, 8, i + 1)\n",
        "    plt.imshow(fm.numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Feature Maps of First Conv Layer (AlexNet)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KReUa0LVxKy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "#like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. CIFAR-10 data loaders (32x32 images)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(96),   # Inception v1 expects larger images; we upscale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# 2. Load GoogLeNet\n",
        "net = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "net.fc = nn.Linear(net.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "def evaluate(loader):\n",
        "    net.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # in case of aux logits\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    net.train()\n",
        "    return correct / total\n",
        "\n",
        "# 3. Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_correct, running_total = 0, 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        if isinstance(outputs, tuple):\n",
        "            outputs = outputs[0]\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        running_total += labels.size(0)\n",
        "        running_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = running_correct / running_total\n",
        "    val_acc = evaluate(testloader)\n",
        "    train_acc_history.append(train_acc)\n",
        "    val_acc_history.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "          f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# 4. Plot training vs validation accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs+1), train_acc_history, label='Train Acc')\n",
        "plt.plot(range(1, num_epochs+1), val_acc_history, label='Val Acc')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"GoogLeNet on CIFAR-10\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cvNPARxxq-x",
        "outputId": "cb1154d4-311f-4e7b-b86e-3c7b9ef0345b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Acc: 0.8429, Val Acc: 0.9170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10\n",
        "\n",
        "X-ray classification: normal vs pneumonia vs COVID-19\n",
        "\n",
        "Given limited labeled data, the best practical approach is:\n",
        "\n",
        "Suggested approach: Transfer learning with a ResNet (e.g., ResNet-50)\n",
        "\n",
        "Why ResNet (or Inception variant)?\n",
        "\n",
        "Pre-trained on ImageNet: Already knows rich visual features useful even for medical images (edges, textures, patterns).\n",
        "\n",
        "Residual connections allow training deeper models without vanishing gradients.\n",
        "\n",
        "ResNet is widely used, well-tested, and has strong libraries and tools.\n",
        "\n",
        "Inception variants are also good, but ResNet is simpler to fine-tune and often used as a strong baseline.\n",
        "\n",
        "Steps for model development\n",
        "\n",
        "Data preparation\n",
        "\n",
        "Collect X-ray images labeled as normal, pneumonia, COVID-19.\n",
        "\n",
        "Use standard splits: train/validation/test.\n",
        "\n",
        "Apply strong data augmentation suitable for X-rays:\n",
        "\n",
        "Small rotations, flips (only if clinically acceptable), zoom, contrast adjustments.\n",
        "\n",
        "Avoid unrealistic transformations.\n",
        "\n",
        "Model setup (transfer learning)\n",
        "\n",
        "Load a pre-trained ResNet50 with ImageNet weights.\n",
        "\n",
        "Replace the final fully connected layer with a new dense layer of size 3 (softmax) for 3 classes.\n",
        "\n",
        "Freeze most layers initially; train only the new classifier on your dataset.\n",
        "\n",
        "Then unfreeze the top few residual blocks and fine-tune with a low learning rate.\n",
        "\n",
        "Training\n",
        "\n",
        "Use cross-entropy loss, Adam or SGD with momentum.\n",
        "\n",
        "Monitor metrics: accuracy, precision/recall, F1-score, and especially sensitivity for pneumonia/COVID-19 (to reduce false negatives).\n",
        "\n",
        "Use early stopping and learning rate scheduling to avoid overfitting.\n",
        "\n",
        "Evaluation & validation\n",
        "\n",
        "Evaluate on a held-out test set and, if possible, perform k-fold cross-validation.\n",
        "\n",
        "Use confusion matrices and ROC curves per class.\n",
        "\n",
        "Involve radiologists for qualitative feedback on misclassified cases."
      ],
      "metadata": {
        "id": "Km9jrDH8yHeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "img_size = (224, 224)\n",
        "batch_size = 16\n",
        "num_classes = 3\n",
        "\n",
        "train_dir = \"xray/train\"\n",
        "val_dir = \"xray/val\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=img_size, batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir, target_size=img_size, batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False,\n",
        "                      input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_gen, epochs=10, validation_data=val_gen)\n",
        "\n",
        "# Fine-tune\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fine_history = model.fit(train_gen, epochs=5, validation_data=val_gen)\n"
      ],
      "metadata": {
        "id": "v9X7RiF-xzEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqbB4_a7WVs7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}